{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707ea0e2-ece3-43a5-8cee-ae3c2fb491f4",
   "metadata": {},
   "source": [
    "# Experiment: Train DistilBERT Model Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7006621-50bd-4e71-ab7d-46fe72a4597f",
   "metadata": {},
   "source": [
    "## Confirm Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334b7bda-1ffe-467b-b90c-16752289eced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : northeastern\n",
      "    active env location : /home/curtis/anaconda3/envs/northeastern\n",
      "            shell level : 2\n",
      "       user config file : /home/curtis/.condarc\n",
      " populated config files : /home/curtis/anaconda3/.condarc\n",
      "          conda version : 24.9.2\n",
      "    conda-build version : 24.9.0\n",
      "         python version : 3.12.7.final.0\n",
      "                 solver : libmamba (default)\n",
      "       virtual packages : __archspec=1=skylake\n",
      "                          __conda=24.9.2=0\n",
      "                          __glibc=2.39=0\n",
      "                          __linux=6.6.87.2=0\n",
      "                          __unix=0=0\n",
      "       base environment : /home/curtis/anaconda3  (writable)\n",
      "      conda av data dir : /home/curtis/anaconda3/etc/conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "          package cache : /home/curtis/anaconda3/pkgs\n",
      "                          /home/curtis/.conda/pkgs\n",
      "       envs directories : /home/curtis/anaconda3/envs\n",
      "                          /home/curtis/.conda/envs\n",
      "               platform : linux-64\n",
      "             user-agent : conda/24.9.2 requests/2.32.3 CPython/3.12.7 Linux/6.6.87.2-microsoft-standard-WSL2 ubuntu/24.04.1 glibc/2.39 solver/libmamba conda-libmamba-solver/24.9.0 libmambapy/1.5.8 aau/0.4.4 c/. s/. e/.\n",
      "                UID:GID : 1000:1000\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741bd87c-ad14-4814-b3f1-ce3cbb905ce8",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b7ad86-254e-44c2-b68a-9feeab1af1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 19:23:24.145524: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 19:23:24.161328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751498604.173263    1800 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751498604.177114    1800 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751498604.188136    1800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751498604.188156    1800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751498604.188157    1800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751498604.188158    1800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-02 19:23:24.191286: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package wordnet to /home/curtis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/curtis/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from emolex.preprocessing import load_mental_health_sentiment_dataset, clean_text, encode_sentiment_labels, split_data, dl_text_vectorization\n",
    "from emolex.train import train_distilbert_model\n",
    "from emolex.evaluation import plot_training_history, generate_confusion_matrix, generate_classification_report\n",
    "from emolex.utils import detect_and_set_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced5de7-e66a-450a-bc22-ae77d08f872e",
   "metadata": {},
   "source": [
    "## Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d992bf9-63ad-4e96-8c28-9dc342e1429e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU devices found despite TensorFlow being built with CUDA. Using CPU.\n",
      "TensorFlow is configured to use: CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 19:23:32.613700: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Detect and set up GPU or use CPU\n",
    "device_used = detect_and_set_device()\n",
    "print(f\"TensorFlow is configured to use: {device_used}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f81300-c1c4-4a4d-a6e3-58f1dd5a8a95",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c38cc6-1003-4040-8cdd-34a131691e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51093 entries, 0 to 51092\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    51093 non-null  object\n",
      " 1   label   51093 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 798.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've shifted my focus to something else but I'...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm restless and restless, it's been a month n...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0                                         oh my gosh  Anxiety\n",
       "1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
       "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
       "3  I've shifted my focus to something else but I'...  Anxiety\n",
       "4  I'm restless and restless, it's been a month n...  Anxiety"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_mental_health_sentiment_dataset()\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee75bc-0560-4765-8c98-69e53bb210ac",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d37bbc0d-e422-4ca3-8360-bbfee7612a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cleaning Text ---\n",
      "Text cleaning complete. Sample cleaned text:\n",
      "\n",
      "                                                     text  \\\n",
      "47817  how to deal with previous stressed situations?...   \n",
      "13616  I just want to die I am going to do it, I am n...   \n",
      "4006   Say! I want to buy a memory card, any suggesti...   \n",
      "919    Clean up your sandals, maybe later on Eid some...   \n",
      "33418                           where are the band-aids?   \n",
      "\n",
      "                                              clean_text  \n",
      "47817  deal previous stressed situation stress since ...  \n",
      "13616  want die going going successful love girlfrien...  \n",
      "4006   say want buy memory card suggestion worth sams...  \n",
      "919    clean sandal maybe later eid someone invite wa...  \n",
      "33418                                           bandaids  \n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Cleaning Text ---\")\n",
    "df['clean_text'] = df[\"text\"].apply(clean_text)\n",
    "print(\"Text cleaning complete. Sample cleaned text:\")\n",
    "print(\"\\n\", df[[\"text\", \"clean_text\"]].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dda799-e583-417c-9670-ab72c20fa083",
   "metadata": {},
   "source": [
    "## Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c7351c-4862-4401-8048-d1b659fb2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Encoding Labels ---\n",
      "Label Encoding Map: {'Anxiety': 0, 'Bipolar': 1, 'Depression': 2, 'Normal': 3, 'Personality disorder': 4, 'Stress': 5, 'Suicidal': 6}\n",
      "Label encoding complete. Sample encoded labels:\n",
      "\n",
      "             label  label_encoded\n",
      "8367     Suicidal              6\n",
      "10879    Suicidal              6\n",
      "43633      Normal              3\n",
      "29804      Normal              3\n",
      "11375  Depression              2\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Encoding Labels ---\")\n",
    "df, encoder = encode_sentiment_labels(df)\n",
    "print(\"Label encoding complete. Sample encoded labels:\")\n",
    "print(\"\\n\", df[['label', 'label_encoded']].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6782c76f-4c69-49d9-bc97-7b8195df974c",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c70d1162-6e0b-47fd-b62c-c1997e0362e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Perform Train-Test Split ---\n",
      "Train set size: 40874 samples\n",
      "Test set size: 10219 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Perform Train-Test Split ---\")\n",
    "X_train_raw, X_test_raw, y_train, y_test = split_data(df) \n",
    "print(f\"Train set size: {len(X_train_raw)} samples\")\n",
    "print(f\"Test set size: {len(X_test_raw)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bddf0f4-35f6-4a3c-936c-f53d79a8627b",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8030b483-993f-42a0-a6da-6174adc3b5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DistilBERT tokenizer...\n",
      "Creating Hugging Face Datasets from input data...\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ece4cd4f77f45ddaa41acd7b1d2c115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958d6ab3d6524a8cb99c2110f8088045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring labels are native integers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc071b9abc084342bdd1584803b1bd24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1663505125e9450ea1eaefa49f03800d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting dataset format to TensorFlow...\n",
      "Creating data collator...\n",
      "Converting Hugging Face Datasets to TensorFlow Datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/anaconda3/envs/northeastern/lib/python3.10/site-packages/datasets/arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n",
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TFDistilBertForSequenceClassification model with 7 labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x70d216f39330>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_distilbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/grad_school/northeastern/ie7500/project/NLP_Project/src/emolex/train.py:320\u001b[0m, in \u001b[0;36mtrain_distilbert_model\u001b[0;34m(X_train, y_train, X_test, y_test, num_classes, max_len, batch_size, epochs, learning_rate, random_seed)\u001b[0m\n\u001b[1;32m    314\u001b[0m model \u001b[38;5;241m=\u001b[39m TFDistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    316\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39mnum_classes\n\u001b[1;32m    317\u001b[0m )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompiling model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 320\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Change this line:\u001b[39;49;00m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optimizer=\"adam\",\u001b[39;49;00m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparseCategoricalCrossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting DistilBERT model training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    329\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    330\u001b[0m     tf_train,\n\u001b[1;32m    331\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtf_test,\n\u001b[1;32m    332\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m    333\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39mclass_weight_dict\n\u001b[1;32m    334\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/northeastern/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1547\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[0;32m-> 1547\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1559\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m   1560\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1567\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/northeastern/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/northeastern/lib/python3.10/site-packages/tf_keras/src/optimizers/__init__.py:335\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get(\n\u001b[1;32m    331\u001b[0m         config,\n\u001b[1;32m    332\u001b[0m         use_legacy_optimizer\u001b[38;5;241m=\u001b[39muse_legacy_optimizer,\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x70d216f39330>"
     ]
    }
   ],
   "source": [
    "model, history = train_distilbert_model(X_train_raw, y_train, X_test_raw, y_test, len(encoder.classes_), epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941043d-f31a-4a24-8882-d2a9a9e176bb",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac7579-6581-4b09-aea6-741efcfd4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Plot Training History ---\")\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e442c4b-0b7e-4106-916b-12f7511b7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Predict Test Classes ---\")\n",
    "y_pred = model.predict(X_test_pad_filtered)\n",
    "y_pred_classes = y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70b794-040a-4b88-8b47-b13e8f36c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Generate Confusion Matrix ---\")\n",
    "fig, ax = generate_confusion_matrix(y_test_filtered, y_pred_classes, class_labels=encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe04ed-e12f-498e-ba74-e7a8c250fd00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Generate Classification Report ---\")\n",
    "generate_classification_report(y_test_filtered, y_pred_classes, class_labels=encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df75c2-15e1-4d43-96af-af61d00ec7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
